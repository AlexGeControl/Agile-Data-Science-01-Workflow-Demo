{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sprint 1--Collect and Display Atomic Records"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Up Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PySpark initiated.\n"
     ]
    }
   ],
   "source": [
    "# Initialize PySpark\n",
    "APP_NAME = \"Collect and Display Atomic Records\"\n",
    "\n",
    "# If there is no SparkSession, create the environment\n",
    "try:\n",
    "  sc and spark\n",
    "except NameError as e:\n",
    "  import findspark\n",
    "  findspark.init()\n",
    "  import pyspark\n",
    "  import pyspark.sql\n",
    "\n",
    "  sc = pyspark.SparkContext()\n",
    "  spark = pyspark.sql.SparkSession(sc).builder.appName(APP_NAME).getOrCreate()\n",
    "\n",
    "print(\"PySpark initiated.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ETL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Raw Dataset in CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load from storage:\n",
    "on_time_dataframe = spark.read.format(\n",
    "    'com.databricks.spark.csv'\n",
    ").options(    \n",
    "    header='true',    \n",
    "    treatEmptyValuesAsNulls='true'\n",
    ").load(\n",
    "    '../data/On_Time_On_Time_Performance_2015/On_Time_On_Time_Performance_2015.csv'\n",
    ")\n",
    "\n",
    "on_time_dataframe.registerTempTable(\"on_time_performance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5819079"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract only needed fields:\n",
    "on_time_dataframe = spark.sql(\n",
    "\"\"\"\n",
    "    SELECT  \n",
    "        Year, \n",
    "        Quarter, \n",
    "        Month, \n",
    "        DayofMonth, \n",
    "        DayOfWeek, \n",
    "        FlightDate,  \n",
    "        Carrier, \n",
    "        TailNum, \n",
    "        FlightNum,  \n",
    "        Origin, \n",
    "        OriginCityName, \n",
    "        OriginState,  \n",
    "        Dest, \n",
    "        DestCityName, \n",
    "        DestState,  \n",
    "        DepTime, \n",
    "        cast(DepDelay as float), \n",
    "        cast(DepDelayMinutes as int),  \n",
    "        cast(TaxiOut as float), \n",
    "        cast(TaxiIn as float),  \n",
    "        WheelsOff, \n",
    "        WheelsOn,  \n",
    "        ArrTime, \n",
    "        cast(ArrDelay as float), \n",
    "        cast(ArrDelayMinutes as float),  \n",
    "        cast(Cancelled as int), \n",
    "        cast(Diverted as int),  \n",
    "        cast(ActualElapsedTime as float), \n",
    "        cast(AirTime as float),  \n",
    "        cast(Flights as int), \n",
    "        cast(Distance as float),  \n",
    "        cast(CarrierDelay as float), \n",
    "        cast(WeatherDelay as float),   \n",
    "        cast(NASDelay as float),  \n",
    "        cast(SecurityDelay as float),   \n",
    "        cast(LateAircraftDelay as float),  \n",
    "        CRSDepTime, \n",
    "        CRSArrTime\n",
    "    FROM\n",
    "        on_time_performance\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "on_time_dataframe.registerTempTable(\"on_time_performance\")\n",
    "on_time_dataframe.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dump as Parquet & JSON Lines to Compress"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "on_time_dataframe.toJSON().saveAsTextFile(\n",
    "    \"../data/On_Time_On_Time_Performance_2015/On_Time_On_Time_Performance_2015.jsonl.gz\",    \n",
    "    \"org.apache.hadoop.io.compress.GzipCodec\"    \n",
    ")\n",
    "\n",
    "on_time_dataframe.write.parquet(\n",
    "    \"../data/On_Time_On_Time_Performance_2015/On_Time_On_Time_Performance_2015.parquet\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compare File Sizes in Different Formats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5G ../data/On_Time_On_Time_Performance_2015/On_Time_On_Time_Performance_2015.csv\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "# On time performance dataset in JSON Lines\n",
    "ls -sh \"../data/On_Time_On_Time_Performance_2015/On_Time_On_Time_Performance_2015.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 283M\n",
      "   0 _SUCCESS\n",
      " 15M part-00000.gz\n",
      " 14M part-00001.gz\n",
      " 15M part-00002.gz\n",
      " 15M part-00003.gz\n",
      " 15M part-00004.gz\n",
      " 15M part-00005.gz\n",
      " 14M part-00006.gz\n",
      " 15M part-00007.gz\n",
      " 16M part-00008.gz\n",
      " 14M part-00009.gz\n",
      " 15M part-00010.gz\n",
      " 15M part-00011.gz\n",
      " 15M part-00012.gz\n",
      " 14M part-00013.gz\n",
      " 14M part-00014.gz\n",
      " 15M part-00015.gz\n",
      " 15M part-00016.gz\n",
      " 15M part-00017.gz\n",
      " 14M part-00018.gz\n",
      "8.7M part-00019.gz\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "# On time performance dataset in JSON Lines\n",
    "ls -sh \"../data/On_Time_On_Time_Performance_2015/On_Time_On_Time_Performance_2015.jsonl.gz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 129M\n",
      "   0 _SUCCESS\n",
      "6.5M part-00000-6cf1f2dc-57cd-41da-b2c4-514368f98f18-c000.snappy.parquet\n",
      "6.7M part-00001-6cf1f2dc-57cd-41da-b2c4-514368f98f18-c000.snappy.parquet\n",
      "6.9M part-00002-6cf1f2dc-57cd-41da-b2c4-514368f98f18-c000.snappy.parquet\n",
      "6.8M part-00003-6cf1f2dc-57cd-41da-b2c4-514368f98f18-c000.snappy.parquet\n",
      "6.7M part-00004-6cf1f2dc-57cd-41da-b2c4-514368f98f18-c000.snappy.parquet\n",
      "6.5M part-00005-6cf1f2dc-57cd-41da-b2c4-514368f98f18-c000.snappy.parquet\n",
      "6.4M part-00006-6cf1f2dc-57cd-41da-b2c4-514368f98f18-c000.snappy.parquet\n",
      "6.6M part-00007-6cf1f2dc-57cd-41da-b2c4-514368f98f18-c000.snappy.parquet\n",
      "6.6M part-00008-6cf1f2dc-57cd-41da-b2c4-514368f98f18-c000.snappy.parquet\n",
      "6.7M part-00009-6cf1f2dc-57cd-41da-b2c4-514368f98f18-c000.snappy.parquet\n",
      "6.7M part-00010-6cf1f2dc-57cd-41da-b2c4-514368f98f18-c000.snappy.parquet\n",
      "6.4M part-00011-6cf1f2dc-57cd-41da-b2c4-514368f98f18-c000.snappy.parquet\n",
      "6.9M part-00012-6cf1f2dc-57cd-41da-b2c4-514368f98f18-c000.snappy.parquet\n",
      "6.2M part-00013-6cf1f2dc-57cd-41da-b2c4-514368f98f18-c000.snappy.parquet\n",
      "6.7M part-00014-6cf1f2dc-57cd-41da-b2c4-514368f98f18-c000.snappy.parquet\n",
      "6.6M part-00015-6cf1f2dc-57cd-41da-b2c4-514368f98f18-c000.snappy.parquet\n",
      "6.4M part-00016-6cf1f2dc-57cd-41da-b2c4-514368f98f18-c000.snappy.parquet\n",
      "6.7M part-00017-6cf1f2dc-57cd-41da-b2c4-514368f98f18-c000.snappy.parquet\n",
      "6.6M part-00018-6cf1f2dc-57cd-41da-b2c4-514368f98f18-c000.snappy.parquet\n",
      "3.7M part-00019-6cf1f2dc-57cd-41da-b2c4-514368f98f18-c000.snappy.parquet\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "# On time performance dataset in JSON Lines\n",
    "ls -sh \"../data/On_Time_On_Time_Performance_2015/On_Time_On_Time_Performance_2015.parquet\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read Back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "on_time_dataframe = spark.read.json(\n",
    "    \"../data/On_Time_On_Time_Performance_2015/On_Time_On_Time_Performance_2015.jsonl.gz\"\n",
    ")\n",
    "\n",
    "on_time_dataframe = spark.read.parquet(\n",
    "    \"../data/On_Time_On_Time_Performance_2015/On_Time_On_Time_Performance_2015.parquet\"    \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inject to MongoDB "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set Up Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pymongo\n",
    "import pymongo_spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Important: ACTIVATE Pymongo Spark\n",
    "pymongo_spark.activate()\n",
    "\n",
    "on_time_dataframe = spark.read.parquet(\n",
    "    \"../data/On_Time_On_Time_Performance_2015/On_Time_On_Time_Performance_2015.parquet\"\n",
    ")\n",
    "\n",
    "# Row in RDD has to be converted to dict to avoid https://jira.mongodb.org/browse/HADOOP-276\n",
    "on_time_dict = on_time_dataframe.rdd.map(\n",
    "    lambda row: row.asDict()\n",
    ")\n",
    "\n",
    "on_time_dict.saveToMongoDB(\n",
    "    'mongodb://localhost:27017/agile_data_science.on_time_performance'\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
