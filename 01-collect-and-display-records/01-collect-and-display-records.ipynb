{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sprint 1--Collect and Display Atomic Records"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Up Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize PySpark\n",
    "APP_NAME = \"Collect and Display Atomic Records\"\n",
    "\n",
    "# If there is no SparkSession, create the environment\n",
    "try:\n",
    "  sc and spark\n",
    "except NameError as e:\n",
    "  import findspark\n",
    "  findspark.init()\n",
    "  import pyspark\n",
    "  import pyspark.sql\n",
    "\n",
    "  sc = pyspark.SparkContext()\n",
    "  spark = pyspark.sql.SparkSession(sc).builder.appName(APP_NAME).getOrCreate()\n",
    "\n",
    "print(\"PySpark initiated.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ETL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Raw Dataset in CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load from storage:\n",
    "on_time_dataframe = spark.read.format(\n",
    "    'com.databricks.spark.csv'\n",
    ").options(    \n",
    "    header='true',    \n",
    "    treatEmptyValuesAsNulls='true'\n",
    ").load(\n",
    "    '../data/On_Time_On_Time_Performance_2015/On_Time_On_Time_Performance_2015.csv'\n",
    ")\n",
    "\n",
    "on_time_dataframe.registerTempTable(\"on_time_performance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract only needed fields:\n",
    "on_time_dataframe = spark.sql(\n",
    "\"\"\"\n",
    "    SELECT  \n",
    "        Year, \n",
    "        Quarter, \n",
    "        Month, \n",
    "        DayofMonth, \n",
    "        DayOfWeek, \n",
    "        FlightDate,  \n",
    "        Carrier, \n",
    "        TailNum, \n",
    "        FlightNum,  \n",
    "        Origin, \n",
    "        OriginCityName, \n",
    "        OriginState,  \n",
    "        Dest, \n",
    "        DestCityName, \n",
    "        DestState,  \n",
    "        DepTime, \n",
    "        cast(DepDelay as float), \n",
    "        cast(DepDelayMinutes as int),  \n",
    "        cast(TaxiOut as float), \n",
    "        cast(TaxiIn as float),  \n",
    "        WheelsOff, \n",
    "        WheelsOn,  \n",
    "        ArrTime, \n",
    "        cast(ArrDelay as float), \n",
    "        cast(ArrDelayMinutes as float),  \n",
    "        cast(Cancelled as int), \n",
    "        cast(Diverted as int),  \n",
    "        cast(ActualElapsedTime as float), \n",
    "        cast(AirTime as float),  \n",
    "        cast(Flights as int), \n",
    "        cast(Distance as float),  \n",
    "        cast(CarrierDelay as float), \n",
    "        cast(WeatherDelay as float),   \n",
    "        cast(NASDelay as float),  \n",
    "        cast(SecurityDelay as float),   \n",
    "        cast(LateAircraftDelay as float),  \n",
    "        CRSDepTime, \n",
    "        CRSArrTime\n",
    "    FROM\n",
    "        on_time_performance\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "on_time_dataframe.registerTempTable(\"on_time_performance\")\n",
    "on_time_dataframe.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dump as Parquet & JSON Lines to Compress"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "on_time_dataframe.toJSON().saveAsTextFile(\n",
    "    \"../data/On_Time_On_Time_Performance_2015/On_Time_On_Time_Performance_2015.jsonl.gz\",    \n",
    "    \"org.apache.hadoop.io.compress.GzipCodec\"    \n",
    ")\n",
    "\n",
    "on_time_dataframe.write.parquet(\n",
    "    \"../data/On_Time_On_Time_Performance_2015/On_Time_On_Time_Performance_2015.parquet\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compare File Sizes in Different Formats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "# On time performance dataset in JSON Lines\n",
    "ls -sh \"../data/On_Time_On_Time_Performance_2015/On_Time_On_Time_Performance_2015.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "# On time performance dataset in JSON Lines\n",
    "ls -sh \"../data/On_Time_On_Time_Performance_2015/On_Time_On_Time_Performance_2015.jsonl.gz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "# On time performance dataset in JSON Lines\n",
    "ls -sh \"../data/On_Time_On_Time_Performance_2015/On_Time_On_Time_Performance_2015.parquet\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read Back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "on_time_dataframe = spark.read.json(\n",
    "    \"../data/On_Time_On_Time_Performance_2015/On_Time_On_Time_Performance_2015.jsonl.gz\"\n",
    ")\n",
    "\n",
    "on_time_dataframe = spark.read.parquet(\n",
    "    \"../data/On_Time_On_Time_Performance_2015/On_Time_On_Time_Performance_2015.parquet\"    \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inject into MongoDB "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set Up Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pymongo\n",
    "import pymongo_spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Important: ACTIVATE Pymongo Spark\n",
    "pymongo_spark.activate()\n",
    "\n",
    "# Load dataset:\n",
    "on_time_dataframe = spark.read.parquet(\n",
    "    \"../data/On_Time_On_Time_Performance_2015/On_Time_On_Time_Performance_2015.parquet\"\n",
    ")\n",
    "\n",
    "# Row in RDD has to be converted to dict to avoid https://jira.mongodb.org/browse/HADOOP-276\n",
    "on_time_dict = on_time_dataframe.rdd.map(\n",
    "    lambda row: row.asDict()\n",
    ")\n",
    "\n",
    "on_time_dict.saveToMongoDB(\n",
    "    'mongodb://localhost:27017/agile_data_science.on_time_performance'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inject into ElasticSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset:\n",
    "on_time_dataframe = spark.read.parquet(\n",
    "    \"../data/On_Time_On_Time_Performance_2015/On_Time_On_Time_Performance_2015.parquet\"\n",
    ")\n",
    "\n",
    "# Save the DataFrame to Elasticsearch\n",
    "on_time_dataframe.write.format(\n",
    "    \"org.elasticsearch.spark.sql\"\n",
    ").option(\n",
    "    \"es.resource\",\"agile_data_science/on_time_performance\"\n",
    ").option(\n",
    "    \"es.batch.size.entries\",\"100\"\n",
    ").mode(\n",
    "    \"overwrite\"\n",
    ").save()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
